# tensorFlow

`Keras`는 `TensorFlow` 의 API이다. `Keras`는 neural network를 정의하기 매우 쉽게 해준다.
"A neural network is basically a set of functions which can learn patterns."  
뉴럴 네트워크의 가장 간단한 형태는 오직 하나의 뉴런만 가진 것이다.  
케라스에서는 `dense`라는 용어를 쓰는데, 이는 연결된 뉴런들의 레이어를 정의하기 위해서이다.

## 뉴런 네트워크

```
model = keras.Sequential([keras.layers.Dense(units = 1, input_shape=[1])
```

하나의 dense만 있으니 하나의 layer만 있고, 그 안에 하나의 unit만 있고 그래서 이 줄은 하나의 뉴런이다.  
연속적인 레이어들은 순서대로 정의되는데, 그래서 sequential 이라는 용어를 쓴다.  
머신러닝에는 원래 여러 수학이 필요하지만 텐서플로우와 케라스는 내부적으로 구현이 되어 있어서 간단하다.

다만 알아놔야할 두 가지 함수는 바로 loss function과 optimizer이다.

## loss function, optimizer

```
model.compile(optimizer="sgd", loss="mean_squared_error")
```

이 함수 둘을 설명하기 위해서 Y = X - 1로 되어있는 데이터가 있다고 가정해보자. 뉴럴 네트워크는 X와 Y의 관계를 모르니 `guess` 를 둔다.
예를 들어 Y = 10X - 10 정도로 예상한다.  
그리고 정답을 알고 있는(정답의) 데이터를 사용하여 이 `guess` 가 얼마나 좋은지 얼마나 나쁜지를 측정한다.  
`loss function` 은 이것을 측정해서 이 데이터를 `optimizer` 에게 주어서 다음 예상을 할 수 있게 한다.
그래서 `optimizer` 는 만든 예상이 얼마나 좋았는지 얼마나 나빴는지를 `loss function` 에게 받은 data를 사용해서 생각한다.
그후 로직은 각각의 `guess` 는 그 전보다 좋아야 한다는 것이다.
`guesse` 들이 더 좋아지면 좋아질수록, 정확도는 100%에 가까워진다.
이것을 `convergence` 라고 한다.
여기서 사용된 `optimaizer` 는 `sgd` 인데, 아래를 의미한다.

```
SGD = stands for stochastic gradient descent
```

시나리오에 따라 다른 `optimzier` 들이 더 효과적일 수 있지만 여기서는 sgd를 쓰고 더 알고 싶으면 tensorFlow documentation을 확인해라.

## represent the know data

```
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)
```

np는 `numpy`라는 파이썬 라이브러리이다. data representation을 특히 list로 만들기 더 쉽게 해주는 라이브러리이다.

## training

```
model.fit(xs, ys, epochs=500)
```

`epochs=500`는 이것이 training loop를 500회 돌 것이라는 것을 의미한다.  
Training loop란, 앞서 언급한 것과 같이

1. Make a guess
2. Measure how good or how bad the guesses with the loss function
3. then use the optimizer
4. and the data to make another guess
5. repeat

## 결과

```
print(model.predict([10.0]))
```

모델이 training을 끝내면, `model` 은 `predict` 라는 메소드를 통해 결과를 알려줄 것이다.
이렇게 모든 코드를 작성해서 돌려보자.

```
model = keras.Sequential([keras.layers.Dense(units = 1, input_shape=[1])
model.compile(optimizer="sgd", loss="mean_squared_error")

xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)

print(model.predict([10.0]))
```

10을 넣은 결과값은 우리가 예상한대로 정확히 19가 나와야할 것 같지만(Y = 2X - 1이니깐) 실제 값은 19는 아니고 19에 굉장히 가까운 값이 나오게 된다. 왜 이럴까?

## 값이 정확하게 나오지 않는 두 가지 이유

첫쩨, 엄청 적은 데이터를 사용해서 훈련을 했기 때문. 오직 6개의 점밖에 없다.  
6개의 점은 linear하지만 모든 X에 대해서 Y와의 관계가 `Y = 2X - 1` 이라는 보장이 없다.  
X가 10과 같을 때 Y가 19와 같을 확률은 매우 높지만 `neural network` 는 긍정적이지가 않다(neural network isn't positive).
그래서 뉴럴 네트워크는 현실적(realistic)인 Y값을 찾아낼 것이다. 그리고 이것이 두 번째 이유이다.
둘째, 뉴럴네트워크를 사용할 때는 뉴럴네트워크가 모든 것에대한 정답을 알아내려고 하기 때문에 확률(probability)를 다룬다.  
앞으로 이것을 많이 보게 될 것이고 우리는 정답을 처리(fit)하는 방법을 조정해야만 한다.
